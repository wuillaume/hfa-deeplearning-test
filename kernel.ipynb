{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Nous chargeons d'abord toutes les libraires que nous allons utiliser\nimport pickle\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing import sequence",
      "execution_count": 133,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fbdf88851ea5230c07eb86652467800c695f2702"
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn import grid_search\nimport xgboost\nimport numpy as np\nfrom keras import layers, models, optimizers\nfrom keras.models import Sequential",
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b99cd1d0044858542918b95b0775327191ee2d9"
      },
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\nfrom nltk import RegexpTokenizer,WhitespaceTokenizer",
      "execution_count": 135,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2cf4f7da8fc078e3719cb3153e008042a9a5cb92"
      },
      "cell_type": "markdown",
      "source": "## Chargeons les donnees "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bd2dacfeded280db1dfe237d3d27949823b79f3b"
      },
      "cell_type": "code",
      "source": "train_seq = pd.read_pickle('../input/Data/Learn/sequences.pkl')\ntrain_snt = pd.read_pickle('../input/Data/Learn/sentences.pkl')\ntrain_labels = pd.read_pickle('../input/Data/Learn/labels.pkl')\ndfdict = pd.read_pickle('../input/Data/dict.pkl')",
      "execution_count": 136,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d1f9802735ed615de6d1b40e69afe6837d1e1280"
      },
      "cell_type": "code",
      "source": "test_seq = pd.read_pickle('../input/Data/Test/sequences_t.pkl')\ntest_snt = pd.read_pickle('../input/Test/sentences_t.pkl')",
      "execution_count": 137,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f77efff02824b8326d08144e54b7f3d6672d8efd"
      },
      "cell_type": "code",
      "source": "tok2 = RegexpTokenizer(r'''(?x)\n          \\d+(\\.\\d+)?\\s*%   # les pourcentages\n        | \\w'               # les contractions d', l', ...\n        | \\w+               # les mots pleins\n        | [^\\w\\s]           # les ponctuations\n        ''')\ntoknizer = RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')",
      "execution_count": 138,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6321e2760d9b6b2c1c59ef78cf9916fb7fee2e11"
      },
      "cell_type": "code",
      "source": "from nltk.stem.snowball import FrenchStemmer\nstemmer = FrenchStemmer()",
      "execution_count": 139,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "5f89446180b693164e940f3996aa8dd277bad46c"
      },
      "cell_type": "code",
      "source": "\ndef stem_sentences(d):\n    tokens = toknizer.tokenize(d)\n    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n    return (' ').join(stemmed_tokens)\n\ndef tokenize_arr(dt):\n    newdt = []\n    for d in dt:\n        newdt.append(stem_sentences(d))\n    return newdt\n\n\n",
      "execution_count": 140,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "662d27ca1272ca796ffbe6524ac6ac2edc91c404"
      },
      "cell_type": "code",
      "source": "train_snt_n = tokenize_arr(train_snt)",
      "execution_count": 141,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee969c317a7606c79426627ba7cb3a90c665670c"
      },
      "cell_type": "code",
      "source": "lbl_enc = preprocessing.LabelEncoder()\ntrain_labels_enc = lbl_enc.fit_transform(train_labels)",
      "execution_count": 143,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10942d4181345273625a516d951570059349c16e"
      },
      "cell_type": "code",
      "source": "# Création du dataset de training et de validation sur les phrases\ntrain_snt_x, valid_snt_x, train_y, valid_y = train_test_split(train_snt_n, train_labels_enc, \n                                                  stratify=train_labels_enc, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d6cda355a1a8090b3441618945848ae627b7ddec"
      },
      "cell_type": "code",
      "source": "# Création du dataset de training et de validation sur les sequences\ntrain_seq_x, valid_seq_x, train_seq_y, valid_seq_y = train_test_split(train_seq, train_labels_enc, \n                                                  stratify=train_labels_enc, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d1d2eeebc22b6106d44dad055057d9eb66f2a7b"
      },
      "cell_type": "code",
      "source": "\nsum =0\nfor lab in train_labels_enc:\n    sum += lab\nprint(sum)\nsum/(len(train_labels))\nn_mitterrand = sum\nn_chirac = len(train_labels) - sum",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "523a29c253a1aa5b077cd4616df3c3ed327a7272"
      },
      "cell_type": "code",
      "source": "sum =0\nfor lab in train_y:\n    sum += lab\nn_mitterrand_train = sum\nn_chirac_train = len(train_y) - n_mitterrand_train",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9cc73274a81ccf3babeffb44c3f534064496eeda"
      },
      "cell_type": "code",
      "source": "print(len(train_snt_x))\nprint(len(train_y))\nprint(n_mitterrand)\nprint(n_mitterrand_train)\nprint(n_chirac_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74bef6a260b14ad1a80b1557f8c6c4abe1f05520"
      },
      "cell_type": "code",
      "source": "index = 0\ntrain_snt_x_reduced = []\ntrain_seq_x_reduced = []\ntrain_y_reduced = []\ncompteur = 0\nfor t in train_y:\n    print(compteur)\n    \n    if(train_y[index]==0):\n        if(compteur<n_mitterrand_train):\n            train_y_reduced.append( train_y[index])\n            train_snt_x_reduced.append(train_snt_x[index])\n            train_seq_x_reduced.append(train_seq_x[index])\n            compteur +=1\n    else:\n        train_snt_x_reduced.append(train_snt_x[index])\n        train_y_reduced.append(train_y[index])\n        train_seq_x_reduced.append(train_seq_x[index])\n    index+=1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a351b0dfeae52fda4453eb77cee093710a9a62f"
      },
      "cell_type": "code",
      "source": "# les données ne sont pas équilibrées entre les classes ´Chirac´ et ´Mitterrand´; je réalise une extension des données de training pour ré-equilibrer.\n\nindex = 0\ntrain_snt_x_extended = []\ntrain_seq_x_extended = []\ntrain_y_extended = []\n\n\ntrain_snt_x_mitterand = []\ntrain_seq_x_mitterand = []\nfor t in train_y:\n    \n    if(train_y[index]==1):\n        train_snt_x_mitterand.append(train_snt_x[index])\n        train_seq_x_mitterand.append(train_seq_x[index])\n    train_snt_x_extended.append(train_snt_x[index])\n    train_seq_x_extended.append(train_seq_x[index])\n    train_y_extended.append(t)\n    index+=1\n\ncompteur = 0 \nwhile(compteur <(n_chirac_train - n_mitterrand_train)):\n    index = compteur % n_mitterrand_train\n    train_snt_x_extended.append(train_snt_x_mitterand[index])\n    train_seq_x_extended.append(train_seq_x_mitterand[index])\n    train_y_extended.append(1)\n    compteur+=1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "11047afa2a4f249b615b671f57e76b131c1c6a36"
      },
      "cell_type": "markdown",
      "source": "# **Etape 1 : Feature engeneering**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7f44657563c589cdd4cfb335f2d355723e8ec55"
      },
      "cell_type": "code",
      "source": "#Feature 1 :  Objet CountVectorize\ncount_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\ncount_vect.fit(train_snt_x_extended + valid_snt_x)\n\ntrain_xcount_aumented =  count_vect.transform(train_snt_x_extended)\nvalid_xcount_aumented =  count_vect.transform(valid_snt_x)\ntest_xcount_aumented = count_vect.transform(test_snt)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "64bfc88041843bc2ba3109c6ac00584eab9aad30"
      },
      "cell_type": "code",
      "source": "#Feature 2 : TF_IDF\n\ntfidf_vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word', \n            vocabulary=dfdict,lowercase=False)\n\ntfidf_vect.fit(list(train_snt_x_extended) + list(valid_snt_x))\ntrain_snt_x_vect =  tfidf_vect.transform(train_snt_x_extended) \nvalid_snt_x_vect = tfidf_vect.transform(valid_snt_x)\ntest_snt_vect = tfidf_vect.transform(test_snt)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5f43f89130a2eacec4c3359b53a1b7265d0ff71f"
      },
      "cell_type": "markdown",
      "source": "\n# Word embedding\n Nous chargeons un modele pre-trained de Fasttext que Facebook fourni pour le francais, nous l´utiliserons pour un layer de nos modeles de neural network"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6fadefdef2079b8e13d3fe9076fd0f90e01c95b9"
      },
      "cell_type": "code",
      "source": "# Nous chargeons le matrices pré entraines sur un corpus grand\nembeddings_index = {}\nfor i, line in enumerate(open('../input/deeplearningtest/wiki.fr.vec')):\n    values = line.strip().split(' ')\n    #print(values)\n    try:\n        embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n    except:\n        print(values)\n        input()",
      "execution_count": 145,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ef2f7a42b586b5221217035bb2fb6432a8e2e3a"
      },
      "cell_type": "code",
      "source": "maxlen = 0\nfor t in train_seq_x_aumented:\n    if(maxlen<len(t)):\n        maxlen = len(t)\n    \nprint(maxlen)",
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": "437\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7b345ffcd8fdb3718b10c266046b4ceb782c8284"
      },
      "cell_type": "code",
      "source": "#\"Pad\" les vecteurs pour une longueur egale\ntrain_seq_x_pad = sequence.pad_sequences(train_seq_x_aumented, maxlen=437)\nvalid_seq_x_pad = sequence.pad_sequences(valid_seq_x, maxlen=437)",
      "execution_count": 147,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa2dae5576b41a0ce63f921cdc9c14a17ca481d4"
      },
      "cell_type": "code",
      "source": "# Nous créons la matrices embedding pour le dictionnaire que nous avons dans nos discours\nembedding_matrix = np.zeros((len(dfdict) + 1, 300))\nfor word, i in dfdict.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector",
      "execution_count": 148,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bfa280d7250e755c0e710f4c3cf52f891f453c3e"
      },
      "cell_type": "markdown",
      "source": "# Etape 2 : Preparation des modèles de classification"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c0c08866b022c9ebdd26a2117ddf83b38428f639"
      },
      "cell_type": "code",
      "source": "# Définition de la fonction de perte\ndef multiclass_logloss(actual, predicted, eps=1e-15):\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b67efb33ba2b7bdfcd3076cda3dbe0a85986beb"
      },
      "cell_type": "code",
      "source": "# Définition de la fonction d'évalutation du modèle\n# Les données étant déséquilibrées je vais privilégier une évaluation de la metrique recall - precision F1 plutot qu'une simple 'accuracy'\ndef resultModel(mod,xtrain,xvalid,ytrain,yvalid):\n    mod.fit(xtrain, ytrain)\n    predictions = mod.predict(xvalid)\n    predictions_proba = mod.predict_proba(xvalid)\n    #print(predictions)\n    print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions_proba))\n    print(\"accuracy :\" + str(metrics.accuracy_score(predictions, yvalid)))\n    print(\"recall :\" + str(metrics.recall_score(predictions, yvalid)))\n    print(\"precision :\" + str(metrics.precision_score(predictions, yvalid)))\n    f1 = 2 * (metrics.precision_score(predictions, yvalid)*metrics.recall_score(predictions, yvalid)/(metrics.recall_score(predictions, yvalid) + metrics.precision_score(predictions, yvalid)))\n    print(\"F1 :\" + str(f1))\n    return mod",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f52561639a43277b60b6f8134947cc77fc146b82"
      },
      "cell_type": "markdown",
      "source": "# Benchmark de modèle"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7bfb58b7b8d80867b304006539342435ac3ce0a4"
      },
      "cell_type": "code",
      "source": "# Regression linéaire\nprint(\"LR avec vector count\")\nmodLR_v = resultModel(LogisticRegression(C=1.0),train_xcount,valid_xcount,train_y_aumented,valid_y)\nprint(\"LR avec tfidf\")\nmodLR_tfidf = resultModel(LogisticRegression(C=1.0),train_snt_x_vect,valid_snt_x_vect,train_y_aumented,valid_y)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "83b20e1ec40f79ec18be9946e8f6541a7e10db05"
      },
      "cell_type": "code",
      "source": "# Naive Bayes\nprint(\"NB avec vector count\")\nresultModel(MultinomialNB(),train_xcount,valid_xcount,train_y_aumented,valid_y)\nprint(\"NB avec tfidf\")\nresultModel(MultinomialNB(),train_snt_x_vect,valid_snt_x_vect,train_y_aumented,valid_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06b2b331411c83f6ddad79a2a1976a2a84400f89"
      },
      "cell_type": "code",
      "source": "#SVM avec parametres fixes\nprint(\"SVM avec vector count\")\nresultModel(SVC(C=1.0, probability=True),train_xcount_aumented,valid_xcount_aumented,train_y_aumented,valid_y)\nprint(\"SVM avec tfidf\")\nresultModel(SVC(C=1.0, probability=True),train_snt_x_vect,valid_snt_x_vect,train_y,valid_y)\n\n### Pour améliorer le modèle SVM nous pourrions réaliser une recherche de paramètres optimaux avec une gridsearch",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "462b61c03a25c162068cbe862fc99ab85b3865d7"
      },
      "cell_type": "markdown",
      "source": "Je vais exporter les résultats du modèle linéaire qui me donne les meilleurs performances"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a25a055f3fc20a23e7a893fe63df9cfd791dd0d"
      },
      "cell_type": "code",
      "source": "predictionsLR_encode = modLR_v.predict(test_xcount)\npredictionsLR_decode = lbl_enc.inverse_transform(predictionsLR_encode)\npredictionsLR_decode.tolist()\n\nwith open('labels_lr.pkl', 'wb') as handle:\n    pickle.dump(predictionsLR_decode, handle, protocol=pickle.HIGHEST_PROTOCOL)",
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'modLR_v' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-149-8b381e4b3122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictionsLR_encode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodLR_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_xcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictionsLR_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlbl_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictionsLR_encode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredictionsLR_decode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels_lr.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'modLR_v' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "c55024f8bc308c32f8b3004cdd3591213686f527"
      },
      "cell_type": "markdown",
      "source": "# Etape 3 : Elaboration d'un modèle de Deep Learning"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "edbf8e3ce7d259b7191111f6b0493541ac0c4e97"
      },
      "cell_type": "code",
      "source": "def resultModelDeep(mod,xtrain,xvalid,ytrain,yvalid):\n    mod.fit(xtrain, ytrain,batch_size=1024, epochs=1,validation_data=(xvalid, yvalid))\n    predictions = mod.predict(xvalid).argmax(axis=-1)\n    print(\"accuracy :\" + str(metrics.accuracy_score(predictions, yvalid)))\n    print(\"recall :\" + str(metrics.recall_score(predictions, yvalid)))\n    print(\"precision :\" + str(metrics.precision_score(predictions, yvalid)))\n    f1 = 2 * (metrics.precision_score(predictions, yvalid)*metrics.recall_score(predictions, yvalid)/(metrics.recall_score(predictions, yvalid) + metrics.precision_score(predictions, yvalid)))\n    print(\"F1 :\" + str(f1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "70060a13ae48abcbe89d6c924c16a1ce27bb65ad"
      },
      "cell_type": "code",
      "source": "def cnn():\n    input_layer = layers.Input((437, ))\n\n    # Le premier \"Layer\" est celui du word embedding dont on a chargé la matrice\n    embedding_layer = layers.Embedding(len(dfdict) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n    # Convolutional Layer\n    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n\n    # Pooling Layer\n    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n\n    # Hidden Layers Dense\n    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\n    output_layer2 = layers.Dense(1, activation=\"softmax\")(output_layer1)\n    \n    model = models.Model(inputs=input_layer, outputs=output_layer2)\n    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n    \n    return model\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9b1e74de374c2b4b3ff4524714e2299e302ebad5"
      },
      "cell_type": "code",
      "source": "# Avant d'entrainer le modèle nous allons scaler les données\nscl = preprocessing.StandardScaler()\ntrain_seq_x_pad = scl.fit_transform(train_seq_x_pad)\nvalid_seq_x_pad = scl.transform(valid_seq_x_pad)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c8b78e9fefc8c537f0605762aa7c33c35465d6f"
      },
      "cell_type": "code",
      "source": "print(\"CNN, Word Embeddings\")\n\nresultModelDeep(cnn(), train_seq_x_pad,valid_seq_x_pad,np.asarray(train_y_aumented),valid_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "90c44b259f34c7c3f968a7177ec0d7492eed7381"
      },
      "cell_type": "markdown",
      "source": "Les resultats ne sont pas convaincants, une seule classe est predicte. Ou bien le pre-train word embedding ne peut pas servir a prédire le propriétaire du discours, ou bien il faut realiser une étude plus poussée sur les paramètres du modèle de neural network pour obtenir de meilleurs resultats."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d8470edc321127f958d3413e6419ef37e793b858"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}